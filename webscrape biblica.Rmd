---
title: "Lecture Code"
author: "Rochelle Terman"
date: "2021"
output:
  html_document: default
  pdf_document: default
editor_options: 
  chunk_output_type: console
---

## Scraping Presidential Statements

To demonstrate webscraping in R, we're going to collect records on presidential statements here: https://www.presidency.ucsb.edu/

Let's say we're interested in how presidents speak about "space exploration". On the website, we punch in this search term, and we get the [following 346 results](https://www.presidency.ucsb.edu/advanced-search?field-keywords=%22space+exploration%22&field-keywords2=&field-keywords3=&from%5Bdate%5D=&to%5Bdate%5D=&person2=&items_per_page=100). 

Our goal is to scrape these records, and store pertinent information in a dataframe. We will be doing this in two steps:

1. Write a function to scrape each individual record page (these notes).
2. Use this function to loop through all results, and collect all pages (homework).

Load the following packages to get started:

```{r warning=FALSE, message=FALSE}
library(tidyverse)
library(rvest)
library(stringr)
library(purrr)
library(knitr)
library(lubridate)
```

Biblica.com 

```{r biblica.com}
biblica_webscrap <- read_html("https://www.biblica.com/bible/niv/genesis/1/")

#Let's take a look at the object we just created
biblica_webscrap
```

```{r}
bibletext <- data.frame(matrix(ncol= 4, nrow =0))
colnames(bibletext) <- c("book","chapter", "verse", "text")
```

Webscrape

```{r}
biblica_webscrap <- read_html("https://www.biblica.com/bible/niv/genesis/1/")
```


```{r book chapter}


book_chapter <- html_nodes(biblica_webscrap, "h1") %>%
  html_text() %>% #select text of element
  str_replace("\\s[:punct:]\\sNew International Version \\(NIV\\)" , "")

```



```{r, text}
#Text


verse_text <- html_nodes(biblica_webscrap, ".verse-span") %>%
  html_text() %>%
  str_replace("(^\\d+)", "@\\1&") %>%
  str_flatten(collapse = " ") %>%
  str_split("@")

book_verse <- data.frame(book_chapter, verse_text) %>%
  as_tibble()

colnames(book_verse) <- c("book_chap", "verse_text")

book_verse <- book_verse %>%
  filter(verse_text != "")

```

```{r}
# Spread and gather are complements
df <- data.frame(x = c("a", "b"), y = c(3, 4), z = c(5, 6))
df
df %>% spread(x, y) %>% gather("x", "y", a:b, na.rm = TRUE)
```










### Challenge 1: Make a function

Make a function called `scrape_docs` that accepts a URL of an individual document, scrapes the page, and returns a list containing the document's date, speaker, title, and full text.

This involves:

- Requesting the HTML of the webpage using the full URL and RVest.
- Using RVest to locate all elements on the page we want to save.
- Storing each of these items into a list.
- Returning this list.

```{r eval = F}
scrape_docs <- function(URL){

  # YOUR CODE HERE
  
}

# uncomment to test
# scrape_doc("https://www.presidency.ucsb.edu/documents/letter-t-keith-glennan-administrator-national-aeronautics-and-space-administration")
```
